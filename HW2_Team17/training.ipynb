{"cells":[{"cell_type":"code","source":"import tensorflow as tf","metadata":{"tags":[],"cell_id":"f27781a74cd64a5998d23e43a866698f","source_hash":"7a93dab8","execution_start":1671572580888,"execution_millis":8457,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2022-12-20 21:43:00.850160: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-12-20 21:43:01.456686: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2022-12-20 21:43:01.539396: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2022-12-20 21:43:01.539429: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2022-12-20 21:43:01.646276: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2022-12-20 21:43:05.150134: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2022-12-20 21:43:05.150193: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2022-12-20 21:43:05.150198: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport numpy as np\nimport random\n\nseed = 42\nos.environ['PYTHONHASHSEED'] = str(seed)\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\nrandom.seed(seed)\ntf.random.set_seed(seed)\nnp.random.seed(seed)","metadata":{"tags":[],"cell_id":"f7154a3cc81b4b94b378729bfe466982","source_hash":"6148f199","execution_start":1671572589391,"execution_millis":0,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":2},{"cell_type":"code","source":"PREPROCESSING_ARGS = {\n    'downsampling_rate': 16000,\n    'frame_lenght_in_s': 0.032,\n    'frame_step_in_s':0.032,\n    'num_mel_bins': 25,\n    'lower_f': 0,\n    'upper_f': 8000,\n    'num_coefficients': 13\n}\n\nTRAINING_ARGS = {\n    'batch_size': 8,\n    'epochs': 20,\n    'initial_learning_rate': 0.01,\n    'end_learning_rate': 1.e-5,\n}\n\n\n\nalpha = 0.7\ninitial_sparsity = 0.5\nfinal_sparsity = 0.85","metadata":{"tags":[],"cell_id":"076bca2bb63c4b6589c7f44355c21544","source_hash":"2a7429a0","execution_start":1671572589443,"execution_millis":7,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":3},{"cell_type":"code","source":"go_stop_train_ds = tf.data.Dataset.list_files(['msc-train/go*', 'msc-train/stop*'])\ngo_stop_val_ds = tf.data.Dataset.list_files(['msc-val/go*', 'msc-val/stop*'])\ngo_stop_test_ds = tf.data.Dataset.list_files(['msc-test/go*', 'msc-test/stop*'])","metadata":{"tags":[],"cell_id":"c1b180d7dd8641bfbaa3b489746b40fa","source_hash":"17d168e2","execution_start":1671572589553,"execution_millis":587,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2022-12-20 21:43:09.449423: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2022-12-20 21:43:09.449467: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n2022-12-20 21:43:09.449486: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (p-f6dc309a-5f8d-46e7-b3a6-26eb71e83e3d): /proc/driver/nvidia/version does not exist\n2022-12-20 21:43:09.449795: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"num_train_files = len(go_stop_train_ds)\nnum_val_files = len(go_stop_val_ds)\nnum_test_files = len(go_stop_test_ds)\n\nprint('Training set size:', num_train_files)\nprint('Validation set size:', num_val_files)\nprint('Test set size:', num_test_files)","metadata":{"tags":[],"cell_id":"28e9be48eaea4d1abae52c925c440846","source_hash":"eadd08a7","execution_start":1671572590151,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Training set size: 1600\nValidation set size: 200\nTest set size: 200\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from preprocessing import get_mfccs\nfrom functools import partial\n\nLABELS = ['go', 'stop']\n\nget_frozen_mfccs = partial(get_mfccs, **PREPROCESSING_ARGS)\n\nfor x,y in go_stop_train_ds.map(get_frozen_mfccs).take(1):\n    SHAPE = x.shape\n\ndef preprocess(filename):\n    signal, label = get_frozen_mfccs(filename)\n    signal.set_shape(SHAPE)\n    signal = tf.expand_dims(signal, -1)\n    signal = tf.image.resize(signal, [32, 32])\n    label_id = tf.argmax(label == LABELS)\n    #print(type(signal))\n    signal = tf.cast(signal, tf.float16)\n    label_id = tf.cast(label_id, tf.int8)\n\n    return signal, label_id","metadata":{"tags":[],"cell_id":"de2c13d24db24b188dbb2e4b6fca8300","source_hash":"a20b7d25","execution_start":1671572590200,"execution_millis":6452,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2022-12-20 21:43:11.661388: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n2022-12-20 21:43:11.661627: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX2 AVX512F FMA\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-20 21:43:13.941489: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-20 21:43:13.945094: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-20 21:43:13.945426: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"batch_size = TRAINING_ARGS['batch_size']\nepoch = TRAINING_ARGS['epochs']\n\n\n\ntrain_ds = go_stop_train_ds.map(preprocess).batch(batch_size).cache()\nval_ds = go_stop_val_ds.map(preprocess).batch(batch_size).cache()\ntest_ds = go_stop_test_ds.map(preprocess).batch(batch_size).cache()","metadata":{"tags":[],"cell_id":"e1cef125ed304a26afdcc717bceb0e25","source_hash":"ad44d325","execution_start":1671572596660,"execution_millis":5990,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-20 21:43:17.745795: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-20 21:43:17.748312: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-20 21:43:17.748550: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-20 21:43:19.661131: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-20 21:43:19.742207: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-20 21:43:19.742427: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-20 21:43:21.462523: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-20 21:43:21.463741: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-20 21:43:21.463900: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"for example_batch, example_labels in train_ds.take(1):\n    print(example_batch.shape)\n    print(example_labels)","metadata":{"tags":[],"cell_id":"fd8155357ba64e71ba83f6b522da8f33","source_hash":"13c3b63a","execution_start":1671572602653,"execution_millis":1643,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"(8, 32, 32, 1)\ntf.Tensor([0 1 0 1 0 0 0 0], shape=(8,), dtype=int8)\n2022-12-20 21:43:24.143358: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=example_batch.shape[1:]),\n    tf.keras.layers.Conv2D(filters=int(128 * alpha), kernel_size=[3, 3], strides=[2, 1],\n        use_bias=False, padding='valid'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1],\n        use_bias=False, padding='same'),\n    tf.keras.layers.Conv2D(filters=int(64 * alpha), kernel_size=[3, 3], strides=[1, 1],\n        use_bias=False, padding='same'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1],\n        use_bias=False, padding='same'),\n    tf.keras.layers.Conv2D(filters=int(64 * alpha), kernel_size=[3, 3], strides=[1, 1],\n        use_bias=False, padding='same'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dense(units=2),\n    tf.keras.layers.Softmax()\n])","metadata":{"tags":[],"cell_id":"95a271483cb944908e0f10542f6b8958","source_hash":"e6c2148c","execution_start":1671572604292,"execution_millis":462,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import tensorflow_model_optimization as tfmot\n\nprune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n\nbegin_step = int(len(train_ds) * epoch * 0.2)\nend_step = int(len(train_ds) * epoch)\n\npruning_params = {\n    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n        initial_sparsity=initial_sparsity,\n        final_sparsity=final_sparsity,\n        begin_step=begin_step,\n        end_step=end_step,\n    )\n}\n\nmodel_for_pruning = prune_low_magnitude(model, **pruning_params)","metadata":{"tags":[],"cell_id":"c02c93e632ec441094020fb3c31345a9","source_hash":"854efcfa","execution_start":1671572604758,"execution_millis":3793,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":10},{"cell_type":"code","source":"model_for_pruning.summary(), tf.keras.utils.plot_model(model_for_pruning, show_shapes=True)","metadata":{"tags":[],"cell_id":"e261ba4040414b438986406e598a5141","source_hash":"5de5f39e","execution_start":1671572608553,"execution_millis":304,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n prune_low_magnitude_conv2d   (None, 15, 30, 89)       1604      \n (PruneLowMagnitude)                                             \n                                                                 \n prune_low_magnitude_batch_n  (None, 15, 30, 89)       357       \n ormalization (PruneLowMagni                                     \n tude)                                                           \n                                                                 \n prune_low_magnitude_re_lu (  (None, 15, 30, 89)       1         \n PruneLowMagnitude)                                              \n                                                                 \n prune_low_magnitude_depthwi  (None, 15, 30, 89)       802       \n se_conv2d (PruneLowMagnitud                                     \n e)                                                              \n                                                                 \n prune_low_magnitude_conv2d_  (None, 15, 30, 44)       70490     \n 1 (PruneLowMagnitude)                                           \n                                                                 \n prune_low_magnitude_batch_n  (None, 15, 30, 44)       177       \n ormalization_1 (PruneLowMag                                     \n nitude)                                                         \n                                                                 \n prune_low_magnitude_re_lu_1  (None, 15, 30, 44)       1         \n  (PruneLowMagnitude)                                            \n                                                                 \n prune_low_magnitude_depthwi  (None, 15, 30, 44)       397       \n se_conv2d_1 (PruneLowMagnit                                     \n ude)                                                            \n                                                                 \n prune_low_magnitude_conv2d_  (None, 15, 30, 44)       34850     \n 2 (PruneLowMagnitude)                                           \n                                                                 \n prune_low_magnitude_batch_n  (None, 15, 30, 44)       177       \n ormalization_2 (PruneLowMag                                     \n nitude)                                                         \n                                                                 \n prune_low_magnitude_re_lu_2  (None, 15, 30, 44)       1         \n  (PruneLowMagnitude)                                            \n                                                                 \n prune_low_magnitude_global_  (None, 44)               1         \n average_pooling2d (PruneLow                                     \n Magnitude)                                                      \n                                                                 \n prune_low_magnitude_dense (  (None, 2)                180       \n PruneLowMagnitude)                                              \n                                                                 \n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"loss = tf.losses.SparseCategoricalCrossentropy(from_logits=False)\n\ninitial_learning_rate = TRAINING_ARGS['initial_learning_rate']\nend_learning_rate = TRAINING_ARGS['end_learning_rate']\n\nlinear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate=initial_learning_rate,\n    end_learning_rate=end_learning_rate,\n    decay_steps=len(train_ds) * epoch,\n)\noptimizer = tf.optimizers.Adam(learning_rate=linear_decay)\nmetrics = [tf.metrics.SparseCategoricalAccuracy()]\ncallbacks = [tfmot.sparsity.keras.UpdatePruningStep()]\n\nmodel_for_pruning.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n#model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n\nhistory = model_for_pruning.fit(train_ds, epochs=epoch, validation_data=val_ds, callbacks=callbacks)\n#history = model.fit(train_ds, epochs=epoch, validation_data=val_ds)","metadata":{"tags":[],"cell_id":"806ab5c4466847fab7a5e8cab9ec7f9c","source_hash":"a8b0bf08","execution_start":1671572608898,"execution_millis":424858,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Epoch 1/20\n200/200 [==============================] - 47s 154ms/step - loss: 0.6245 - sparse_categorical_accuracy: 0.6637 - val_loss: 0.6233 - val_sparse_categorical_accuracy: 0.7400\nEpoch 2/20\n200/200 [==============================] - 23s 115ms/step - loss: 0.4366 - sparse_categorical_accuracy: 0.7987 - val_loss: 0.2514 - val_sparse_categorical_accuracy: 0.9050\nEpoch 3/20\n200/200 [==============================] - 22s 112ms/step - loss: 0.2903 - sparse_categorical_accuracy: 0.8900 - val_loss: 0.2332 - val_sparse_categorical_accuracy: 0.9200\nEpoch 4/20\n200/200 [==============================] - 23s 115ms/step - loss: 0.2029 - sparse_categorical_accuracy: 0.9256 - val_loss: 0.2552 - val_sparse_categorical_accuracy: 0.8850\nEpoch 5/20\n200/200 [==============================] - 23s 115ms/step - loss: 0.1670 - sparse_categorical_accuracy: 0.9431 - val_loss: 0.1841 - val_sparse_categorical_accuracy: 0.9500\nEpoch 6/20\n200/200 [==============================] - 23s 116ms/step - loss: 0.1393 - sparse_categorical_accuracy: 0.9488 - val_loss: 0.1399 - val_sparse_categorical_accuracy: 0.9600\nEpoch 7/20\n200/200 [==============================] - 23s 118ms/step - loss: 0.1252 - sparse_categorical_accuracy: 0.9494 - val_loss: 0.1350 - val_sparse_categorical_accuracy: 0.9600\nEpoch 8/20\n200/200 [==============================] - 20s 98ms/step - loss: 0.1070 - sparse_categorical_accuracy: 0.9588 - val_loss: 0.1398 - val_sparse_categorical_accuracy: 0.9700\nEpoch 9/20\n200/200 [==============================] - 11s 52ms/step - loss: 0.0868 - sparse_categorical_accuracy: 0.9656 - val_loss: 0.1046 - val_sparse_categorical_accuracy: 0.9550\nEpoch 10/20\n200/200 [==============================] - 18s 90ms/step - loss: 0.0794 - sparse_categorical_accuracy: 0.9712 - val_loss: 0.1312 - val_sparse_categorical_accuracy: 0.9700\nEpoch 11/20\n200/200 [==============================] - 23s 116ms/step - loss: 0.0737 - sparse_categorical_accuracy: 0.9719 - val_loss: 0.1425 - val_sparse_categorical_accuracy: 0.9600\nEpoch 12/20\n200/200 [==============================] - 23s 116ms/step - loss: 0.0596 - sparse_categorical_accuracy: 0.9787 - val_loss: 0.1409 - val_sparse_categorical_accuracy: 0.9500\nEpoch 13/20\n200/200 [==============================] - 23s 115ms/step - loss: 0.0554 - sparse_categorical_accuracy: 0.9787 - val_loss: 0.1946 - val_sparse_categorical_accuracy: 0.9350\nEpoch 14/20\n200/200 [==============================] - 15s 75ms/step - loss: 0.0575 - sparse_categorical_accuracy: 0.9794 - val_loss: 0.1511 - val_sparse_categorical_accuracy: 0.9450\nEpoch 15/20\n200/200 [==============================] - 14s 71ms/step - loss: 0.0458 - sparse_categorical_accuracy: 0.9837 - val_loss: 0.1118 - val_sparse_categorical_accuracy: 0.9700\nEpoch 16/20\n200/200 [==============================] - 12s 59ms/step - loss: 0.0420 - sparse_categorical_accuracy: 0.9825 - val_loss: 0.0947 - val_sparse_categorical_accuracy: 0.9700\nEpoch 17/20\n200/200 [==============================] - 11s 53ms/step - loss: 0.0360 - sparse_categorical_accuracy: 0.9856 - val_loss: 0.1042 - val_sparse_categorical_accuracy: 0.9700\nEpoch 18/20\n200/200 [==============================] - 10s 53ms/step - loss: 0.0349 - sparse_categorical_accuracy: 0.9869 - val_loss: 0.1011 - val_sparse_categorical_accuracy: 0.9750\nEpoch 19/20\n200/200 [==============================] - 11s 53ms/step - loss: 0.0334 - sparse_categorical_accuracy: 0.9900 - val_loss: 0.1029 - val_sparse_categorical_accuracy: 0.9750\nEpoch 20/20\n200/200 [==============================] - 10s 52ms/step - loss: 0.0319 - sparse_categorical_accuracy: 0.9894 - val_loss: 0.1023 - val_sparse_categorical_accuracy: 0.9650\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"test_loss, test_accuracy = model_for_pruning.evaluate(test_ds)\n#test_loss, test_accuracy = model.evaluate(test_ds)","metadata":{"tags":[],"cell_id":"ba3a40e1e3574d70809f1f1ee50c4e06","source_hash":"f48c4339","execution_start":1671573033757,"execution_millis":1109,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"25/25 [==============================] - 1s 26ms/step - loss: 0.0569 - sparse_categorical_accuracy: 0.9750\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"training_loss = history.history['loss'][-1]\ntraining_accuracy = history.history['sparse_categorical_accuracy'][-1]\nval_loss = history.history['val_loss'][-1]\nval_accuracy = history.history['val_sparse_categorical_accuracy'][-1]\n\nprint(f'Training loss: {training_loss:.3f}')\nprint(f'Training accuracy: {training_accuracy * 100:.2f}%')\nprint(f'Validation loss: {val_loss:.3f}')\nprint(f'Validation accuracy: {val_accuracy * 100:.2f}%')\nprint(f'Test loss: {test_loss:.3f}')\nprint(f'Test accuracy: {test_accuracy * 100:.2f}%')","metadata":{"tags":[],"cell_id":"3a9c609ec08d4afba736f04661377506","source_hash":"584ea5fc","execution_start":1671573034868,"execution_millis":5,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Training loss: 0.032\nTraining accuracy: 98.94%\nValidation loss: 0.102\nValidation accuracy: 96.50%\nTest loss: 0.057\nTest accuracy: 97.50%\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from time import time\ntimestamp = time()\nMODEL_NAME = int(timestamp)","metadata":{"tags":[],"cell_id":"574cc5828ee84705a44935b61f6598ff","source_hash":"83b909bb","execution_start":1671573034872,"execution_millis":44,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":15},{"cell_type":"code","source":"saved_model_dir = f'saved_models/{MODEL_NAME}'\n\nif not os.path.exists(saved_model_dir):\n    os.makedirs(saved_model_dir)\n\nmodel.save(saved_model_dir)","metadata":{"tags":[],"cell_id":"1d31a7404ac34a338a853a2acc208e88","source_hash":"692e554c","execution_start":1671573034916,"execution_millis":1019,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: saved_models/1671573034/assets\nINFO:tensorflow:Assets written to: saved_models/1671573034/assets\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"#!ls saved_models/","metadata":{"tags":[],"cell_id":"1773fc302b9f4c3b94cc3766324ab8a2","source_hash":"ca5f9950","execution_start":1671573035934,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":17},{"cell_type":"code","source":"converter = tf.lite.TFLiteConverter.from_saved_model(f'./saved_models/{MODEL_NAME}')\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\ntflite_model = converter.convert()","metadata":{"tags":[],"cell_id":"bcdfa2791d2645f784b47a845f793a3c","source_hash":"7ed5d5e1","execution_start":1671573035937,"execution_millis":458,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2022-12-20 21:50:36.254068: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n2022-12-20 21:50:36.254103: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n2022-12-20 21:50:36.254633: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./saved_models/1671573034\n2022-12-20 21:50:36.257430: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n2022-12-20 21:50:36.257473: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: ./saved_models/1671573034\n2022-12-20 21:50:36.263328: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n2022-12-20 21:50:36.264661: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n2022-12-20 21:50:36.297985: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: ./saved_models/1671573034\n2022-12-20 21:50:36.307784: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 53153 microseconds.\n2022-12-20 21:50:36.327950: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"tflite_models_dir = './tflite_models'\n\nif not os.path.exists(tflite_models_dir):\n    os.makedirs(tflite_models_dir)\n\ntflite_model_name = os.path.join(tflite_models_dir, f'{MODEL_NAME}.tflite')\ntflite_model_name","metadata":{"tags":[],"cell_id":"0902b977efe34d5194b2d892f6698c7c","source_hash":"5018f86f","execution_start":1671573036400,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"'./tflite_models/1671573034.tflite'"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"with open(tflite_model_name, 'wb') as fp:\n    fp.write(tflite_model)","metadata":{"tags":[],"cell_id":"c0c4c08dc9d1420ea99252071a59227d","source_hash":"90231985","execution_start":1671573036401,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":20},{"cell_type":"code","source":"downsampling_rate = PREPROCESSING_ARGS['downsampling_rate']\nsampling_rate_int64 = tf.cast(downsampling_rate, tf.int64)\nframe_length = int(downsampling_rate * PREPROCESSING_ARGS['frame_lenght_in_s'])\nframe_step = int(downsampling_rate * PREPROCESSING_ARGS['frame_step_in_s'])\nnum_spectrogram_bins = frame_length // 2 + 1\nnum_mel_bins = PREPROCESSING_ARGS['num_mel_bins']\nlower_frequency = PREPROCESSING_ARGS['lower_f']\nupper_frequency = PREPROCESSING_ARGS['upper_f']\nnum_coefficients = PREPROCESSING_ARGS['num_coefficients']\n\nlinear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n    num_mel_bins=num_mel_bins,\n    num_spectrogram_bins=num_spectrogram_bins,\n    sample_rate=downsampling_rate,\n    lower_edge_hertz=lower_frequency,\n    upper_edge_hertz=upper_frequency\n)","metadata":{"tags":[],"cell_id":"ced1e9d6a46548088e64096b5772e750","source_hash":"ff5f8251","execution_start":1671573036407,"execution_millis":59,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":21},{"cell_type":"code","source":"interpreter = tf.lite.Interpreter(model_path=f'./tflite_models/{MODEL_NAME}.tflite')\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nprint('Number of inputs:', len(input_details))\nprint('Input name:', input_details[0]['name'])\nprint('Input index:', input_details[0]['index'])\nprint('Number of output:', len(output_details))\nprint('Output name:', output_details[0]['name'])\nprint('Output index:', output_details[0]['index'])","metadata":{"tags":[],"cell_id":"e9746ab29d2e47c6904f7886c42d5cd6","source_hash":"4af54417","execution_start":1671573036487,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Number of inputs: 1\nInput name: serving_default_input_1:0\nInput index: 0\nNumber of output: 1\nOutput name: StatefulPartitionedCall:0\nOutput index: 21\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"from glob import glob\npattern = ('./msc-test/go*', './msc-test/stop*')\nfilenames = sum([glob(pat) for pat in pattern], [])\nprint(len(filenames))","metadata":{"tags":[],"cell_id":"0a73775eceac40bd82aea6ce962809c8","source_hash":"5c913c6a","execution_start":1671573036488,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"200\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"from time import time\n\navg_preprocessing_latency = 0\navg_model_latency = 0\nlatencies = []\naccuracy = 0.0\n\nfor filename in filenames:\n    audio_binary = tf.io.read_file(filename)\n\n    # NEED ONLY FOR TESTING\n    path_parts = tf.strings.split(filename, '/')\n    path_end = path_parts[-1]\n    file_parts = tf.strings.split(path_end, '_')\n    true_label = file_parts[0]\n    true_label = true_label.numpy().decode()\n\n    # PRE-PROCESSING (LOG-MEL SPECTROGRAM)\n    start_preprocess = time()\n    audio, sampling_rate = tf.audio.decode_wav(audio_binary)\n    audio = tf.squeeze(audio)\n\n    zero_padding = tf.zeros(sampling_rate - tf.shape(audio), dtype=tf.float32)\n    audio_padded = tf.concat([audio, zero_padding], axis=0)\n\n    if downsampling_rate != sampling_rate:\n        audio_padded = tfio.audio.resample(audio_padded, sampling_rate_int64, downsampling_rate)\n\n    stft = tf.signal.stft(\n        audio_padded,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length\n    )\n    spectrogram = tf.abs(stft)\n\n    mel_spectrogram = tf.matmul(spectrogram, linear_to_mel_weight_matrix)\n    log_mel_spectrogram = tf.math.log(mel_spectrogram + 1.e-6)\n    mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrogram)[...,:num_coefficients]\n    \n    mfccs = tf.expand_dims(mfccs, 0)  # batch axis\n    mfccs = tf.expand_dims(mfccs, -1)  # channel axis\n    mfccs = tf.image.resize(mfccs, [32, 32])\n    end_preprocess = time()\n\n    interpreter.set_tensor(input_details[0]['index'], mfccs)\n    interpreter.invoke()\n    output = interpreter.get_tensor(output_details[0]['index'])\n\n    end_inference = time()\n\n    top_index = np.argmax(output[0])\n    predicted_label = LABELS[top_index]\n\n    accuracy += true_label == predicted_label\n    avg_preprocessing_latency += (end_preprocess - start_preprocess)\n    avg_model_latency += (end_inference - end_preprocess)\n    latencies.append(end_inference - start_preprocess)","metadata":{"tags":[],"cell_id":"7b32e487b26c46cd9cd79aed8057c547","source_hash":"95e1d545","execution_start":1671573036489,"execution_millis":2305,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":24},{"cell_type":"code","source":"accuracy /= len(filenames)\navg_preprocessing_latency /= len(filenames)\navg_model_latency /= len(filenames)\ntotal_latency = np.median(latencies)\n\nimport os\n\nmodel_size = os.path.getsize(f'./tflite_models/{MODEL_NAME}.tflite')","metadata":{"tags":[],"cell_id":"dfe5f2308b374d4ebdeff9f7972effac","source_hash":"69c1ab82","execution_start":1671573038798,"execution_millis":18,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":25},{"cell_type":"code","source":"print(f'Accuracy: {accuracy * 100.:.3f}%')\nprint(f'Model size: {model_size / 1024:.1f} KB')\nprint(f'Preprocessing Latency: {1000 * avg_preprocessing_latency:.1f} ms')\nprint(f'Model Latency: {1000 * avg_model_latency:.1f} ms')\nprint(f'Total Latency: {1000 * total_latency:.1f} ms')","metadata":{"tags":[],"cell_id":"e60f3708a32f4e53b181a25a21747ec5","source_hash":"4dec92c6","execution_start":1671573038816,"execution_millis":6,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Accuracy: 97.500%\nModel size: 65.8 KB\nPreprocessing Latency: 5.4 ms\nModel Latency: 0.9 ms\nTotal Latency: 6.1 ms\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import zipfile\n\ntflite_models_dir = './tflite_models'\n\nif not os.path.exists(tflite_models_dir):\n    os.makedirs(tflite_models_dir)\n\ntflite_model_name = os.path.join(tflite_models_dir, f'{MODEL_NAME}.tflite')\n\nwith zipfile.ZipFile(f'{tflite_model_name}.zip', 'w', compression=zipfile.ZIP_DEFLATED) as f:\n    f.write(tflite_model_name)","metadata":{"tags":[],"cell_id":"384d8ef24835492ea509987e39398998","source_hash":"ec5cf548","execution_start":1671573038817,"execution_millis":0,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":27},{"cell_type":"code","source":"tflite_model_name","metadata":{"tags":[],"cell_id":"8ad68ed74404483d80cb3954a566736d","source_hash":"ec125fcd","execution_start":1671573038817,"execution_millis":6,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"'./tflite_models/1671573034.tflite'"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"pruned_tflite_size = os.path.getsize(tflite_model_name) / 1024\npruned_zip_size = os.path.getsize(f'{tflite_model_name}.zip') / 1024\n\nprint(f'Original TFLite Size (pruned model): {pruned_tflite_size:.2f} KB')\nprint(f'ZIP TFLite Size (pruned model): {pruned_zip_size:.2f} KB')","metadata":{"tags":[],"cell_id":"02d6076490964a39a03bd216bc658d13","source_hash":"d544701d","execution_start":1671573038824,"execution_millis":18,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Original TFLite Size (pruned model): 65.78 KB\nZIP TFLite Size (pruned model): 23.66 KB\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"import pandas as pd\nfrom time import time\n\n#timestamp = time()\n\nmodel_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n\noutput_dict = {\n    'timestamp': int(timestamp),\n    **PREPROCESSING_ARGS,\n    **TRAINING_ARGS,\n    'final_sparsity': final_sparsity,\n    'test_accuracy': test_accuracy,\n    'size_zip': pruned_zip_size,\n    'tflite_size': pruned_tflite_size,\n    'train_accuracy': training_accuracy,\n    'latency' : f'{1000*total_latency:.1f} ms',\n    'alpha': alpha\n\n}\noutput_dict\n\ndf = pd.DataFrame([output_dict])","metadata":{"tags":[],"cell_id":"11af2fee5e5c43b2bdfd7411c0ebef83","source_hash":"4bae2f6","execution_start":1671573038883,"execution_millis":43,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":30},{"cell_type":"code","source":"import os\noutput_path = 'MFCCS_pruned_test.csv'\n\ndf.to_csv(output_path, mode='a', header=not (os.path.exists(output_path)), index=False)","metadata":{"tags":[],"cell_id":"7c318ffaa87a4f7195397bf298f020a3","source_hash":"3b42d55","execution_start":1671573038927,"execution_millis":0,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":31},{"cell_type":"code","source":"saved_model_dir = f'saved_models/{MODEL_NAME}'\n\nif not os.path.exists(saved_model_dir):\n    os.makedirs(saved_model_dir)\n\nmodel_for_export.save(saved_model_dir)","metadata":{"tags":[],"cell_id":"c743c83a361d4f2f9c7ea5593dcc2233","source_hash":"dd5ba8eb","execution_start":1671573038927,"execution_millis":1045,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: saved_models/1671573034/assets\nINFO:tensorflow:Assets written to: saved_models/1671573034/assets\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"print(f'Total latency: {1000*total_latency:.1f} ms')\nprint(f'Train Accuracy: {100*training_accuracy:.2f}%')\nprint(f'Size Zipped: {pruned_zip_size:.2f}kb')","metadata":{"tags":[],"cell_id":"1711a3d0663342ba8d60eb698a9d1a0f","source_hash":"17acb256","execution_start":1671573040011,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Total latency: 6.1 ms\nTrain Accuracy: 98.94%\nSize Zipped: 23.66kb\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"57ccacb8ad5f4b8a9924804a60dd2018","source_hash":"b623e53d","execution_start":1671573040012,"execution_millis":0,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=f6dc309a-5f8d-46e7-b3a6-26eb71e83e3d' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"c14cf0abc932428bb6dca4444524532a","deepnote_execution_queue":[]}}